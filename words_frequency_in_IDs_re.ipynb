{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# WordSegment 라이브러리 설치\n",
        "!pip install wordsegment"
      ],
      "metadata": {
        "id": "LFcKhmBSbC6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import defaultdict, Counter  # defaultdict 추가\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordsegment import load, segment\n",
        "\n",
        "# 필요한 데이터 로드 (nltk punkt)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# WordSegment 데이터 로드\n",
        "load()"
      ],
      "metadata": {
        "id": "-gdglr_CcBt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV 파일 불러오기\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "csv_file_path = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(csv_file_path)"
      ],
      "metadata": {
        "id": "RDeyunM4cDw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 빈도 및 해당 단어를 포함하는 ID 목록 계산 함수\n",
        "def calculate_word_frequency_with_ids(ids):\n",
        "    word_freq = Counter()\n",
        "    word_id_map = defaultdict(set)  # defaultdict를 set으로 변경하여 중복 제거\n",
        "\n",
        "    for id_ in ids:\n",
        "        words = word_tokenize(id_)  # ID를 단어 토큰으로 나눔\n",
        "        for word in words:\n",
        "            if word.isalpha():  # 알파벳으로만 이루어진 경우\n",
        "                # 붙어있는 단어를 분리\n",
        "                segmented_words = segment(word.lower())\n",
        "                word_freq.update(segmented_words)\n",
        "                for segmented_word in segmented_words:\n",
        "                    word_id_map[segmented_word].add(id_)  # set에 추가하여 중복 제거\n",
        "\n",
        "    # ID 목록을 다시 리스트로 변환\n",
        "    for word in word_id_map:\n",
        "        word_id_map[word] = list(word_id_map[word])\n",
        "\n",
        "    return word_freq, word_id_map"
      ],
      "metadata": {
        "id": "HBHZQq-QcQ8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ID 컬럼에서 단어 빈도 및 ID 목록 계산\n",
        "ids = df['ID'].astype(str).tolist()  # 'ID' 컬럼 이름을 실제 컬럼 이름으로 변경\n",
        "word_frequencies, word_id_map = calculate_word_frequency_with_ids(ids)\n",
        "\n",
        "# 단어 빈도 데이터를 DataFrame으로 변환\n",
        "word_freq_df = pd.DataFrame(word_frequencies.items(), columns=['Word', 'Frequency'])\n",
        "\n",
        "# 해당 단어를 포함하는 ID 목록을 추가\n",
        "word_freq_df['IDs'] = word_freq_df['Word'].map(word_id_map)\n"
      ],
      "metadata": {
        "id": "OQVUs6S4euCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도가 높은 순서로 정렬\n",
        "word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "# 상위 10개의 빈도 높은 단어 및 해당 단어가 포함된 ID 출력\n",
        "print(word_freq_df.head(10))\n",
        "\n",
        "# CSV 파일로 저장\n",
        "output_csv_file_path = \"word_frequencies_with_ids_output.csv\"\n",
        "word_freq_df.to_csv(output_csv_file_path, index=False)\n",
        "\n",
        "print(f\"Download Complete!\": {output_csv_file_path}\")"
      ],
      "metadata": {
        "id": "Xq1_ZmnDeub_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}